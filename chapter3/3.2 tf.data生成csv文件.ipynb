{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n",
      "sys.version_info(major=3, minor=6, micro=5, releaselevel='final', serial=0)\n",
      "matplotlib 2.2.2\n",
      "numpy 1.19.2\n",
      "pandas 0.23.0\n",
      "sklearn 0.23.2\n",
      "tensorflow 2.0.0\n",
      "tensorflow_core.keras 2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf.__version__)\n",
    "print(sys.version_info)\n",
    "for module in mpl, np, pd, sklearn, tf, keras:\n",
    "    print(module.__name__, module.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20640, 8)\n",
      "(20640,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "print(housing.data.shape)\n",
    "print(housing.target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11610, 8) (11610,)\n",
      "(3870, 8) (3870,)\n",
      "(5160, 8) (5160,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## 划分训练集,验证集和测试集\n",
    "x_train_all, x_test, y_train_all, y_test = train_test_split(\n",
    "    housing.data, housing.target, random_state = 7)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(\n",
    "    x_train_all, y_train_all, random_state = 11)\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_valid.shape, y_valid.shape)\n",
    "print(x_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    " \n",
    "# 进行数据标准化\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_valid_scaled = scaler.transform(x_valid)\n",
    "x_test_scaled = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.data生成csv文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"generate_csv\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "    \n",
    "def save_to_csv(output_dir, \n",
    "                data, \n",
    "                name_prefix,\n",
    "                header=None, \n",
    "                n_parts = 10):\n",
    "    \n",
    "    path_format = os.path.join(output_dir, \"{}_{:02d}.csv\")\n",
    "    filenames = []\n",
    "    \n",
    "    for file_idx, row_indices in enumerate(\n",
    "        # 拆分成n_parts个数组\n",
    "        np.array_split(np.arange(len(data)), n_parts)):\n",
    "        \n",
    "        path_csv = path_format.format(name_prefix, file_idx)\n",
    "        filenames.append(path_csv) \n",
    "        \n",
    "        with open(path_csv, \"wt\", encoding = \"utf-8\") as f:\n",
    "            if header is not None:\n",
    "                f.write(header + \"\\n\")\n",
    "            for row_index in row_indices:\n",
    "                # 输出 \",拼接的字符串\"\n",
    "                f.write(\",\".join([repr(col) for col in data[row_index]]))\n",
    "                f.write(\"\\n\")     \n",
    "    return filenames\n",
    "\n",
    "# np.c_是按行连接两个矩阵，就是把两矩阵左右相加，要求行数相等\n",
    "train_data = np.c_[x_train_scaled, y_train]\n",
    "valid_data = np.c_[x_valid_scaled, y_valid]\n",
    "test_data = np.c_[x_test_scaled, y_test]\n",
    "\n",
    "\n",
    "header_cols = housing.feature_names + [\"MidianHouseValue\"]\n",
    "header_str = \",\".join(header_cols)\n",
    "\n",
    "# 所有训练文件的文件名\n",
    "train_filenames = save_to_csv(output_dir, train_data, \"train\",\n",
    "                             header_str, n_parts=20)\n",
    "valid_filenames = save_to_csv(output_dir, valid_data, \"valid\",\n",
    "                              header_str, n_parts=10)\n",
    "test_filenames = save_to_csv(output_dir, test_data, \"test\",\n",
    "                             header_str, n_parts=10)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.80154431  0.27216142 -0.11624393 ... -0.58976206 -0.08241846\n",
      "   3.226     ]\n",
      " [-0.29807281  0.35226166 -0.10920508 ...  1.08055484 -1.06113817\n",
      "   1.514     ]\n",
      " [-0.03058829 -0.92934213  0.25962148 ...  1.59844639 -1.81515182\n",
      "   1.598     ]\n",
      " ...\n",
      " [-1.11006415 -1.40994355 -0.57897311 ...  1.76174553 -2.13473376\n",
      "   1.5       ]\n",
      " [ 0.32465459  0.27216142 -0.10777932 ... -0.65508172  0.64662786\n",
      "   2.636     ]\n",
      " [-0.10982126 -0.52884094  0.25735571 ... -1.14497913  1.17094199\n",
      "   1.925     ]]\n"
     ]
    }
   ],
   "source": [
    "print(train_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['generate_csv\\\\train_00.csv', 'generate_csv\\\\train_01.csv', 'generate_csv\\\\train_02.csv', 'generate_csv\\\\train_03.csv', 'generate_csv\\\\train_04.csv', 'generate_csv\\\\train_05.csv', 'generate_csv\\\\train_06.csv', 'generate_csv\\\\train_07.csv', 'generate_csv\\\\train_08.csv', 'generate_csv\\\\train_09.csv', 'generate_csv\\\\train_10.csv', 'generate_csv\\\\train_11.csv', 'generate_csv\\\\train_12.csv', 'generate_csv\\\\train_13.csv', 'generate_csv\\\\train_14.csv', 'generate_csv\\\\train_15.csv', 'generate_csv\\\\train_16.csv', 'generate_csv\\\\train_17.csv', 'generate_csv\\\\train_18.csv', 'generate_csv\\\\train_19.csv']\n"
     ]
    }
   ],
   "source": [
    "print(train_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取output_dir所有文件生成一个dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train filenames:\n",
      "['generate_csv\\\\train_00.csv',\n",
      " 'generate_csv\\\\train_01.csv',\n",
      " 'generate_csv\\\\train_02.csv',\n",
      " 'generate_csv\\\\train_03.csv',\n",
      " 'generate_csv\\\\train_04.csv',\n",
      " 'generate_csv\\\\train_05.csv',\n",
      " 'generate_csv\\\\train_06.csv',\n",
      " 'generate_csv\\\\train_07.csv',\n",
      " 'generate_csv\\\\train_08.csv',\n",
      " 'generate_csv\\\\train_09.csv',\n",
      " 'generate_csv\\\\train_10.csv',\n",
      " 'generate_csv\\\\train_11.csv',\n",
      " 'generate_csv\\\\train_12.csv',\n",
      " 'generate_csv\\\\train_13.csv',\n",
      " 'generate_csv\\\\train_14.csv',\n",
      " 'generate_csv\\\\train_15.csv',\n",
      " 'generate_csv\\\\train_16.csv',\n",
      " 'generate_csv\\\\train_17.csv',\n",
      " 'generate_csv\\\\train_18.csv',\n",
      " 'generate_csv\\\\train_19.csv']\n",
      "valid filenames:\n",
      "['generate_csv\\\\valid_00.csv',\n",
      " 'generate_csv\\\\valid_01.csv',\n",
      " 'generate_csv\\\\valid_02.csv',\n",
      " 'generate_csv\\\\valid_03.csv',\n",
      " 'generate_csv\\\\valid_04.csv',\n",
      " 'generate_csv\\\\valid_05.csv',\n",
      " 'generate_csv\\\\valid_06.csv',\n",
      " 'generate_csv\\\\valid_07.csv',\n",
      " 'generate_csv\\\\valid_08.csv',\n",
      " 'generate_csv\\\\valid_09.csv']\n",
      "test filenames:\n",
      "['generate_csv\\\\test_00.csv',\n",
      " 'generate_csv\\\\test_01.csv',\n",
      " 'generate_csv\\\\test_02.csv',\n",
      " 'generate_csv\\\\test_03.csv',\n",
      " 'generate_csv\\\\test_04.csv',\n",
      " 'generate_csv\\\\test_05.csv',\n",
      " 'generate_csv\\\\test_06.csv',\n",
      " 'generate_csv\\\\test_07.csv',\n",
      " 'generate_csv\\\\test_08.csv',\n",
      " 'generate_csv\\\\test_09.csv']\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "print(\"train filenames:\")\n",
    "pprint.pprint(train_filenames)\n",
    "print(\"valid filenames:\")\n",
    "pprint.pprint(valid_filenames)\n",
    "print(\"test filenames:\")\n",
    "pprint.pprint(test_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'generate_csv\\\\train_11.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv\\\\train_03.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv\\\\train_19.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv\\\\train_09.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv\\\\train_16.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv\\\\train_05.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv\\\\train_14.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv\\\\train_10.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv\\\\train_00.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv\\\\train_12.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv\\\\train_15.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv\\\\train_13.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv\\\\train_06.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv\\\\train_08.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv\\\\train_01.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv\\\\train_07.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv\\\\train_18.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv\\\\train_17.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv\\\\train_02.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv\\\\train_04.csv', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "# 读取output_dir所有文件生成一个dataset\n",
    "# tf.data.Dataset.list_files(train_filenames) 读取文件列表生成一个dataset列表\n",
    "\n",
    "filename_dataset = tf.data.Dataset.list_files(train_filenames)\n",
    "for filename in filename_dataset:\n",
    "    print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "interleave()是Dataset的类方法，所以interleave是作用在一个Dataset上的\n",
    "\n",
    "    interleave(\n",
    "        map_func,\n",
    "        cycle_length=AUTOTUNE,\n",
    "        block_length=1,\n",
    "        num_parallel_calls=None\n",
    "    )\n",
    "\n",
    "解释：\n",
    "\n",
    "    假定我们现在有一个Dataset——A\n",
    "    从该A中取出cycle_length个element，然后对这些element apply map_func,得到cycle_length个新的Dataset对象。\n",
    "    然后从这些新生成的Dataset对象中取数据，取数逻辑为轮流从每个对象里面取数据，每次取block_length个数据\n",
    "    当这些新生成的某个Dataset的对象取尽时，从原Dataset中再取cycle_length个element，然后apply map_func，以此类推。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'-0.32652634129448693,0.43236189741438374,-0.09345459539684739,-0.08402991822890092,0.8460035745154013,-0.0266316482653991,-0.5617679242614233,0.1422875991184281,2.431'\n",
      "b'-0.09719300311107498,-1.249743071766074,0.36232962250170797,0.026906080250728295,1.033811814747154,0.045881586971778555,1.3418334617377423,-1.6353869745909178,1.832'\n",
      "b'0.04971034572063198,-0.8492418886278699,-0.06214699417830008,0.17878747064657746,-0.8025354230744277,0.0005066066922077538,0.6466457006743215,-1.1060793768010604,2.286'\n",
      "b'0.09734603446040174,0.7527628439249472,-0.20218964416999152,-0.1954700015215477,-0.4060513603629498,0.006785531677655949,-0.813715166526018,0.656614793197258,1.119'\n",
      "b'0.15782311132800697,0.43236189741438374,0.3379948076652917,-0.015880306122244434,-0.3733890577139493,-0.05305245634489608,0.8006134598360177,-1.2359095422966828,3.169'\n",
      "b'2.2754266257529974,-1.249743071766074,1.0294788075585177,-0.17124431895714504,-0.45413752815175606,0.10527151658164971,-0.9023632702857819,0.9012947204774823,3.798'\n",
      "b'1.0534699704183814,-0.1283397589791022,0.13509497508586193,-0.2852867771449356,-0.37066719915986596,-0.017744041396267323,0.7586222527919203,-1.1510205879341566,2.674'\n",
      "b'-1.453851024367546,1.874166156711919,-1.1315714708271856,0.3611276016530489,-0.3978857847006997,-0.03273859332533962,-0.7390641317809511,0.646627857389904,1.875'\n",
      "b'-1.4803330571456954,-0.6890414153725881,-0.35624704887282904,-0.1725588908792445,-0.8215884329530113,-0.1382309124854157,1.9157132913404298,-1.0211904224385344,0.928'\n",
      "b'2.2878417437355094,-1.8905449647872008,0.6607106467795992,-0.14964778023694128,-0.06672632728722275,0.44788055801575993,-0.5337737862320228,0.5667323709310584,3.59'\n",
      "b'-0.4394346460367383,0.1920611875314612,-0.39172440230167493,-0.06233787211356993,0.682692061270399,-0.012080008421921133,0.935918460311448,-1.2458964781040367,1.618'\n",
      "b'-0.8698076415077927,-0.44874070548966555,0.9621267572121975,3.9409717092762584,-0.9740125119816802,-0.09383082108319943,-0.6690787867074531,1.6752822455475638,0.425'\n",
      "b'-1.1179501498535522,0.3522616607867429,-0.17415480367337632,0.1029357335256435,-0.24364713330264193,-0.06195252491676357,1.9063819119972951,-1.1210597805120879,0.603'\n",
      "b'0.21174628471128154,1.1532640270631513,-0.2507761334605016,-0.2564987121705146,-0.6473894854916754,0.017590216427099285,0.7959477701644521,-1.1510205879341566,1.935'\n",
      "b'0.3798565732727743,-1.5701440182766375,0.4541195259524651,-0.13374802152613807,-0.28356772542919806,-0.04747003172530946,-0.3191520613399599,-0.41698080609349797,1.901'\n"
     ]
    }
   ],
   "source": [
    "# interleave\n",
    "# 跳过头文件读取生成的csv文件内容\n",
    "n_readers = 5\n",
    "dataset = filename_dataset.interleave(\n",
    "#     lambda filename: tf.data.TextLineDataset(filename),\n",
    "    lambda filename: tf.data.TextLineDataset(filename).skip(1),\n",
    "    cycle_length = n_readers  \n",
    ")\n",
    "\n",
    "# 查看前15个\n",
    "for line in dataset.take(15):\n",
    "    print(line.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取生成的csv文件内容\n",
    "\n",
    "用法：\n",
    "\n",
    "    tf.io.decode_csv(str, record_defaults)\n",
    "    \n",
    "    str 字符串\n",
    "    \n",
    "    record_defaults 字符串的类型\n",
    "\n",
    "作用：将CSV记录转换为张量。每列映射到一个张量。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: id=96, shape=(), dtype=int32, numpy=1>, <tf.Tensor: id=97, shape=(), dtype=int32, numpy=2>, <tf.Tensor: id=98, shape=(), dtype=int32, numpy=3>, <tf.Tensor: id=99, shape=(), dtype=int32, numpy=4>, <tf.Tensor: id=100, shape=(), dtype=int32, numpy=5>]\n"
     ]
    }
   ],
   "source": [
    "sample_str = \"1,2,3,4,5\"\n",
    "record_defaults = [tf.constant(0, dtype=tf.int32)] * 5\n",
    "parsed_fields = tf.io.decode_csv(sample_str, record_defaults)\n",
    "print(parsed_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: id=107, shape=(), dtype=int32, numpy=1>, <tf.Tensor: id=108, shape=(), dtype=int32, numpy=2>, <tf.Tensor: id=109, shape=(), dtype=float32, numpy=3.0>, <tf.Tensor: id=110, shape=(), dtype=string, numpy=b'4'>, <tf.Tensor: id=111, shape=(), dtype=float32, numpy=5.0>]\n"
     ]
    }
   ],
   "source": [
    "sample_str = \"1,2,3,4,5\"\n",
    "record_defaults = [tf.constant(0, dtype=tf.int32),\n",
    "                  0,\n",
    "                  np.nan,\n",
    "                  \"hello\",\n",
    "                  tf.constant([])]\n",
    "\n",
    "parsed_fields = tf.io.decode_csv(sample_str, record_defaults)\n",
    "print(parsed_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Field 4 is required but missing in record 0! [Op:DecodeCSV]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    parsed_files = tf.io.decode_csv(',,,,',  record_defaults)\n",
    "except tf.errors.InvalidArgumentError as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expect 5 fields but have 7 in record 0 [Op:DecodeCSV]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    parsed_files = tf.io.decode_csv('1,2,3,4,5,6,7',  record_defaults)\n",
    "except tf.errors.InvalidArgumentError as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: id=131, shape=(8,), dtype=float32, numpy=\n",
       " array([ 0.63636464, -1.0895426 ,  0.09260903, -0.20538124,  1.2025671 ,\n",
       "        -0.03630123, -0.6784102 ,  0.18223535], dtype=float32)>,\n",
       " <tf.Tensor: id=132, shape=(1,), dtype=float32, numpy=array([2.429], dtype=float32)>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 解析csv文件\n",
    "def parse_csv_line(line, n_fields=9):\n",
    "    '''\n",
    "        line：文件\n",
    "        n_fields: 文件个数\n",
    "    '''\n",
    "    defs = [tf.constant(np.nan)] * n_fields\n",
    "    parsed_fields = tf.io.decode_csv(line, record_defaults=defs)\n",
    "    x = tf.stack(parsed_fields[0:-1])\n",
    "    y = tf.stack(parsed_fields[-1:]) # 最后一个值\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "#测试\n",
    "parse_csv_line(b'0.6363646332204844,-1.0895425985107923,0.09260902815633619,-0.20538124656801682,1.2025670451003232,-0.03630122549633783,-0.6784101660505877,0.182235342347858,2.429',\n",
    "               n_fields=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取output_dir所有文件生成一个dataset\n",
    "\n",
    "https://blog.csdn.net/anshuai_aw1/article/details/105094548"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "<tf.Tensor: id=216, shape=(3, 8), dtype=float32, numpy=\n",
      "array([[ 8.1150836e-01, -4.8239522e-02,  5.1873392e-01, -2.9386396e-02,\n",
      "        -3.4064025e-02, -5.0815947e-02, -7.1573567e-01,  9.1627514e-01],\n",
      "       [ 6.3034356e-01,  1.8741661e+00, -6.7132145e-02, -1.2543367e-01,\n",
      "        -1.9737554e-01, -2.2722632e-02, -6.9240725e-01,  7.2652334e-01],\n",
      "       [ 4.9710345e-02, -8.4924191e-01, -6.2146995e-02,  1.7878747e-01,\n",
      "        -8.0253541e-01,  5.0660671e-04,  6.4664572e-01, -1.1060793e+00]],\n",
      "      dtype=float32)>\n",
      "y:\n",
      "<tf.Tensor: id=217, shape=(3, 1), dtype=float32, numpy=\n",
      "array([[2.147],\n",
      "       [2.419],\n",
      "       [2.286]], dtype=float32)>\n",
      "x:\n",
      "<tf.Tensor: id=218, shape=(3, 8), dtype=float32, numpy=\n",
      "array([[ 0.63636464, -1.0895426 ,  0.09260903, -0.20538124,  1.2025671 ,\n",
      "        -0.03630123, -0.6784102 ,  0.18223535],\n",
      "       [-1.1157656 ,  0.99306357, -0.334192  , -0.06535219, -0.32893205,\n",
      "         0.04343066, -0.12785879,  0.30707204],\n",
      "       [-0.69061434, -0.12833975,  7.020181  ,  5.6242876 , -0.2663293 ,\n",
      "        -0.0366208 , -0.64575034,  1.2058963 ]], dtype=float32)>\n",
      "y:\n",
      "<tf.Tensor: id=219, shape=(3, 1), dtype=float32, numpy=\n",
      "array([[2.429],\n",
      "       [0.524],\n",
      "       [1.352]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "# 1. filename -> dataset\n",
    "# 2. read file -> dataset -> datasets -> merge\n",
    "# 3. parse csv\n",
    "def csv_reader_dataset(filenames, \n",
    "                       n_readers=5,\n",
    "                       batch_size=32, \n",
    "                       n_parse_threads=5,\n",
    "                       shuffle_buffer_size=10000):\n",
    "    \n",
    "    # 读取所有的文件名生成dataset\n",
    "    dataset = tf.data.Dataset.list_files(filenames)\n",
    "    #为了配合输出次数，一般默认repeat()空\n",
    "    dataset = dataset.repeat()\n",
    "    # 跳过头行读取所有数据内容\n",
    "    dataset = dataset.interleave(\n",
    "        lambda filename: tf.data.TextLineDataset(filename).skip(1),\n",
    "        cycle_length = n_readers \n",
    "    )\n",
    "    \n",
    "    # 打乱顺序\n",
    "    dataset.shuffle(shuffle_buffer_size)\n",
    "    # map接收一个函数，Dataset中的每个元素都会被当作这个函数的输入，\n",
    "    # 并将函数返回值作为新的Dataset\n",
    "    dataset = dataset.map(parse_csv_line,\n",
    "                          num_parallel_calls=n_parse_threads)\n",
    "    # 一次读取batch_size个数据\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset\n",
    "\n",
    "train_set = csv_reader_dataset(train_filenames, batch_size=3)\n",
    "for x_batch, y_batch in train_set.take(2):\n",
    "    print(\"x:\")\n",
    "    pprint.pprint(x_batch)\n",
    "    print(\"y:\")\n",
    "    pprint.pprint(y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取所有的csv文件\n",
    "batch_size = 32\n",
    "train_set = csv_reader_dataset(train_filenames,\n",
    "                               batch_size = batch_size)\n",
    "valid_set = csv_reader_dataset(valid_filenames,\n",
    "                               batch_size = batch_size)\n",
    "test_set = csv_reader_dataset(test_filenames,\n",
    "                              batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.data读取csv文件并与tf.keras结合使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 362 steps, validate for 120 steps\n",
      "Epoch 1/100\n",
      "362/362 [==============================] - 1s 3ms/step - loss: 0.8284 - val_loss: 0.7309\n",
      "Epoch 2/100\n",
      "362/362 [==============================] - 0s 1ms/step - loss: 0.5346 - val_loss: 0.5284\n",
      "Epoch 3/100\n",
      "362/362 [==============================] - 0s 1ms/step - loss: 0.4677 - val_loss: 0.4737\n",
      "Epoch 4/100\n",
      "362/362 [==============================] - 0s 1ms/step - loss: 0.4422 - val_loss: 0.4570\n",
      "Epoch 5/100\n",
      "362/362 [==============================] - 0s 1ms/step - loss: 0.4213 - val_loss: 0.4364\n",
      "Epoch 6/100\n",
      "362/362 [==============================] - 0s 1ms/step - loss: 0.4192 - val_loss: 0.4232\n",
      "Epoch 7/100\n",
      "362/362 [==============================] - 0s 1ms/step - loss: 0.4108 - val_loss: 0.4370\n",
      "Epoch 8/100\n",
      "362/362 [==============================] - 0s 1ms/step - loss: 0.3989 - val_loss: 0.4090\n",
      "Epoch 9/100\n",
      "362/362 [==============================] - 0s 1ms/step - loss: 0.3911 - val_loss: 0.4019\n",
      "Epoch 10/100\n",
      "362/362 [==============================] - 0s 1ms/step - loss: 0.3850 - val_loss: 0.4493\n",
      "Epoch 11/100\n",
      "362/362 [==============================] - 0s 1ms/step - loss: 0.3853 - val_loss: 0.3884\n",
      "Epoch 12/100\n",
      "362/362 [==============================] - 0s 1ms/step - loss: 0.3763 - val_loss: 0.3975\n",
      "Epoch 13/100\n",
      "362/362 [==============================] - 0s 1ms/step - loss: 0.3758 - val_loss: 0.4127\n",
      "Epoch 14/100\n",
      "362/362 [==============================] - 0s 1ms/step - loss: 0.3702 - val_loss: 0.3957\n",
      "Epoch 15/100\n",
      "362/362 [==============================] - 0s 1ms/step - loss: 0.3726 - val_loss: 0.3839\n",
      "Epoch 16/100\n",
      "362/362 [==============================] - 0s 1ms/step - loss: 0.3834 - val_loss: 0.3792\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation='relu',\n",
    "                       input_shape=[8]),\n",
    "    keras.layers.Dense(1),\n",
    "])\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=\"sgd\")\n",
    "callbacks = [keras.callbacks.EarlyStopping(\n",
    "    patience=5, min_delta=1e-2)]\n",
    "\n",
    "history = model.fit(train_set,\n",
    "                    validation_data = valid_set,\n",
    "                    steps_per_epoch = len(x_train) // batch_size,\n",
    "                    validation_steps = len(x_valid) // batch_size,\n",
    "                    epochs = 100,\n",
    "                    callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161/161 [==============================] - 0s 1ms/step - loss: 0.4578\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.45776501066566255"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_set, steps = len(x_test) // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1, shape=(), dtype=int64)\n",
      "tf.Tensor(1, shape=(), dtype=int64)\n",
      "tf.Tensor(1, shape=(), dtype=int64)\n",
      "tf.Tensor(1, shape=(), dtype=int64)\n",
      "tf.Tensor(2, shape=(), dtype=int64)\n",
      "tf.Tensor(2, shape=(), dtype=int64)\n",
      "tf.Tensor(2, shape=(), dtype=int64)\n",
      "tf.Tensor(2, shape=(), dtype=int64)\n",
      "tf.Tensor(1, shape=(), dtype=int64)\n",
      "tf.Tensor(1, shape=(), dtype=int64)\n",
      "tf.Tensor(2, shape=(), dtype=int64)\n",
      "tf.Tensor(2, shape=(), dtype=int64)\n",
      "tf.Tensor(3, shape=(), dtype=int64)\n",
      "tf.Tensor(3, shape=(), dtype=int64)\n",
      "tf.Tensor(3, shape=(), dtype=int64)\n",
      "tf.Tensor(3, shape=(), dtype=int64)\n",
      "tf.Tensor(4, shape=(), dtype=int64)\n",
      "tf.Tensor(4, shape=(), dtype=int64)\n",
      "tf.Tensor(4, shape=(), dtype=int64)\n",
      "tf.Tensor(4, shape=(), dtype=int64)\n",
      "tf.Tensor(3, shape=(), dtype=int64)\n",
      "tf.Tensor(3, shape=(), dtype=int64)\n",
      "tf.Tensor(4, shape=(), dtype=int64)\n",
      "tf.Tensor(4, shape=(), dtype=int64)\n",
      "tf.Tensor(5, shape=(), dtype=int64)\n",
      "tf.Tensor(5, shape=(), dtype=int64)\n",
      "tf.Tensor(5, shape=(), dtype=int64)\n",
      "tf.Tensor(5, shape=(), dtype=int64)\n",
      "tf.Tensor(5, shape=(), dtype=int64)\n",
      "tf.Tensor(5, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "a = tf.data.Dataset.range(1, 6)  # ==> [ 1, 2, 3, 4, 5 ]\n",
    "# NOTE: New lines indicate \"block\" boundaries.\n",
    "b=a.interleave(lambda x: tf.data.Dataset.from_tensors(x).repeat(6),\n",
    "            cycle_length=2, block_length=4) \n",
    "\n",
    "\n",
    "for item in b:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
